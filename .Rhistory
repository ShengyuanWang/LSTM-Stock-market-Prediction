testPredict <- model %>%
predict(testXY$dataX,
verbose = 1)
trainPredict <- trainPredict * spread + min_value
testPredict <- testPredict * spread + min_value
# Plot the output
df <- data.frame(
index = 1:length(dataset),
value = dataset * spread + min_value,
type = 'raw'
) %>%
rbind(data.frame(
index = 1:length(trainPredict) + look_back,
value = trainPredict,
type = 'train'
)) %>%
rbind(data.frame(
index = 1:length(testPredict) + look_back + length(train),
value = testPredict,
type = 'test'
))
ggplot(data = df) +
geom_line(mapping = aes(x = index,
y = value,
color = type)) +
geom_vline(xintercept = length(train) + 0.5) +
theme_minimal()
df %>%
mutate(index = ifelse(type %in% c("test", "train"), index - 2, index)) %>%
filter(index >= 1500,
index <= 1550) %>%
ggplot(aes(x = index,
y = value,
color = type)) +
geom_line()
df %>%
mutate(index = ifelse(type %in% c("test", "train"), index - 2, index)) %>%
filter(index >= 1500,
index <= 1550) %>%
ggplot(aes(x = index,
y = value,
color = type)) +
geom_line()
# Save the predicted numbers as X-2
index <- c(1,2,3)
value <- c(0,0,0)
empty_data_1 <- cbind(index,value)
index <- c(1820,1821,1822,1823,1824,1825,1826)
value <- c(0,0,0,0,0,0,0)
empty_data_2 <- cbind(index,value)
result <- df %>%
mutate(index = ifelse(type %in% c("test", "train"), index - 2, index)) %>%
filter(type %in% c("test", "train")) %>%
select(index, value) %>%
rbind(empty_data_1) %>%
mutate(index = ifelse(index >= 1458, index - 5, index)) %>%
rbind(empty_data_2) %>%
arrange(index)
bitcoin_result <- bitcoin_data %>%
cbind(result) %>%
select(Date, value) %>%
filter(value != 0)
bitcoin_result
write.csv(bitcoin_result, "prediction/bitcoin_LSTM_result_modified.csv", row.names = FALSE)
df %>%
mutate(index = ifelse(type %in% c("test", "train"), index - 2, index)) %>%
filter(index >= 1500,
index <= 1550) %>%
ggplot(aes(x = index,
y = value,
color = type)) +
geom_line()
# Seperate training data and testing data
train <- dataset[1:train_size]
test <- dataset[(train_size + 1):length(dataset)]
look_back <- 5
trainXY <- create_dataset(train, look_back)
testXY <-  create_dataset(test, look_back)
dim_train <- dim(trainXY$dataX)
dim_test <- dim(testXY$dataX)
# reshape input to be [samples, time steps, features]
dim(trainXY$dataX) <- c(dim_train[1], 1, dim_train[2])
dim(testXY$dataX) <- c(dim_test[1], 1, dim_test[2])
# Initialize the model
model <- keras_model_sequential()
# Train the model
trained_model <- model %>%
layer_lstm(units = 4,
input_shape = c(1, look_back),
dropout = 0.2) %>%
layer_dense(units = 1) %>%
compile(loss = 'mean_squared_error',
optimizer = 'adam') %>%
fit(
trainXY$dataX,
trainXY$dataY,
epochs = 15,
batch_size = 15,
verbose = 1,
validation_split = 0.25
)
plot(trained_model)
# Initialize the model
model <- keras_model_sequential()
# Train the model
trained_model <- model %>%
layer_lstm(units = 4,
input_shape = c(1, look_back),
dropout = 0.2) %>%
layer_dense(units = 1) %>%
compile(loss = 'mean_squared_error',
optimizer = 'adam') %>%
fit(
trainXY$dataX,
trainXY$dataY,
epochs = 15,
batch_size = 97,
verbose = 1,
validation_split = 0.25
)
plot(trained_model)
# Initialize the model
model <- keras_model_sequential()
# Train the model
trained_model <- model %>%
layer_lstm(units = 4,
input_shape = c(1, look_back),
dropout = 0.2) %>%
layer_dense(units = 1) %>%
compile(loss = 'mean_squared_error',
optimizer = 'adam') %>%
fit(
trainXY$dataX,
trainXY$dataY,
epochs = 30,
batch_size = 97,
verbose = 1,
validation_split = 0.25
)
plot(trained_model)
# Print out the test score
trainScore <- model %>%
evaluate(
trainXY$dataX,
trainXY$dataY,
verbose = 2)
testScore <- model %>%
evaluate(
testXY$dataX,
testXY$dataY,
verbose = 2)
sprintf(
'Train Score: %.4f MSE (%.4f RMSE)',
trainScore * spread^2,
sqrt(trainScore) * spread)
sprintf(
'Test Score: %.4f MSE (%.4f RMSE)',
testScore * spread^2,
sqrt(testScore) * spread)
# Initialize the model
model <- keras_model_sequential()
# Train the model
trained_model <- model %>%
layer_lstm(units = 2,
input_shape = c(1, look_back),
dropout = 0.2) %>%
layer_dense(units = 1) %>%
compile(loss = 'mean_squared_error',
optimizer = 'adam') %>%
fit(
trainXY$dataX,
trainXY$dataY,
epochs = 30,
batch_size = 15,
verbose = 1,
validation_split = 0.25
)
plot(trained_model)
# Initialize the model
model <- keras_model_sequential()
# Train the model
trained_model <- model %>%
layer_lstm(units = 2,
input_shape = c(1, look_back),
dropout = 0.2) %>%
layer_dense(units = 1) %>%
compile(loss = 'mean_squared_error',
optimizer = 'adam') %>%
fit(
trainXY$dataX,
trainXY$dataY,
epochs = 50,
batch_size = 15,
verbose = 1,
validation_split = 0.25
)
plot(trained_model)
# Seperate training data and testing data
train <- dataset[1:train_size]
test <- dataset[(train_size + 1):length(dataset)]
look_back <- 5
trainXY <- create_dataset(train, look_back)
testXY <-  create_dataset(test, look_back)
dim_train <- dim(trainXY$dataX)
dim_test <- dim(testXY$dataX)
# reshape input to be [samples, time steps, features]
dim(trainXY$dataX) <- c(dim_train[1], 1, dim_train[2])
dim(testXY$dataX) <- c(dim_test[1], 1, dim_test[2])
# Initialize the model
model <- keras_model_sequential()
# Train the model
trained_model <- model %>%
layer_lstm(units = 4,
input_shape = c(1, look_back),
dropout = 0.2) %>%
layer_dense(units = 1) %>%
compile(loss = 'mean_squared_error',
optimizer = 'adam') %>%
fit(
trainXY$dataX,
trainXY$dataY,
epochs = 50,
batch_size = 15,
verbose = 1,
validation_split = 0.25
)
# Print out the test score
trainScore <- model %>%
evaluate(
trainXY$dataX,
trainXY$dataY,
verbose = 2)
testScore <- model %>%
evaluate(
testXY$dataX,
testXY$dataY,
verbose = 2)
sprintf(
'Train Score: %.4f MSE (%.4f RMSE)',
trainScore * spread^2,
sqrt(trainScore) * spread)
sprintf(
'Test Score: %.4f MSE (%.4f RMSE)',
testScore * spread^2,
sqrt(testScore) * spread)
# Fit the model to get predicted value
trainPredict <- model %>%
predict(trainXY$dataX,
verbose = 1)
testPredict <- model %>%
predict(testXY$dataX,
verbose = 1)
trainPredict <- trainPredict * spread + min_value
testPredict <- testPredict * spread + min_value
# Initialize the model
model <- keras_model_sequential()
# Train the model
trained_model <- model %>%
layer_lstm(units = 4,
input_shape = c(1, look_back),
dropout = 0.2) %>%
layer_dense(units = 1) %>%
compile(loss = 'mean_squared_error',
optimizer = 'adam') %>%
fit(
trainXY$dataX,
trainXY$dataY,
epochs = 50,
batch_size = 97,
verbose = 2,
validation_split = 0.25
)
plot(trained_model)
# Print out the test score
trainScore <- model %>%
evaluate(
trainXY$dataX,
trainXY$dataY,
verbose = 2)
testScore <- model %>%
evaluate(
testXY$dataX,
testXY$dataY,
verbose = 2)
sprintf(
'Train Score: %.4f MSE (%.4f RMSE)',
trainScore * spread^2,
sqrt(trainScore) * spread)
sprintf(
'Test Score: %.4f MSE (%.4f RMSE)',
testScore * spread^2,
sqrt(testScore) * spread)
# Fit the model to get predicted value
trainPredict <- model %>%
predict(trainXY$dataX,
verbose = 1)
testPredict <- model %>%
predict(testXY$dataX,
verbose = 1)
trainPredict <- trainPredict * spread + min_value
testPredict <- testPredict * spread + min_value
# Plot the output
df <- data.frame(
index = 1:length(dataset),
value = dataset * spread + min_value,
type = 'raw'
) %>%
rbind(data.frame(
index = 1:length(trainPredict) + look_back,
value = trainPredict,
type = 'train'
)) %>%
rbind(data.frame(
index = 1:length(testPredict) + look_back + length(train),
value = testPredict,
type = 'test'
))
ggplot(data = df) +
geom_line(mapping = aes(x = index,
y = value,
color = type)) +
geom_vline(xintercept = length(train) + 0.5) +
theme_minimal()
df %>%
mutate(index = ifelse(type %in% c("test", "train"), index - 2, index)) %>%
filter(index >= 1500,
index <= 1550) %>%
ggplot(aes(x = index,
y = value,
color = type)) +
geom_line()
df %>%
filter(index >= 1500,
index <= 1550) %>%
ggplot(aes(x = index,
y = value,
color = type)) +
geom_line()
library(keras)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(lubridate)
#install_keras()
theme_set(theme_minimal())
# Load the data & clean the data
bitcoin_data <- read.csv("data/BCHAIN-MKPRU.csv") %>%
mutate(Date = as.Date(Date, "%m/%d/%y")) %>%
na.omit()
# Normalize the whole dataset
max_value <- max(bitcoin_data$Value)
min_value <- min(bitcoin_data$Value)
spread <- max_value - min_value
dataset <- (bitcoin_data$Value - min_value) / spread
create_dataset <- function(dataset,
look_back = 5)
{
l <- length(dataset)
dataX <- array(dim = c(l - look_back, look_back))
for (i in 1:ncol(dataX))
{
dataX[, i] <- dataset[i:(l - look_back + i - 1)]
}
dataY <- array(data = dataset[(look_back + 1):l],
dim = c(l - look_back, 1))
return(list(dataX = dataX,
dataY = dataY))
}
# Set the size of train and test dataset
train_size <- as.integer(length(dataset) * 0.8)
test_size <- length(dataset) - train_size
# Seperate training data and testing data
train <- dataset[1:train_size]
test <- dataset[(train_size + 1):length(dataset)]
look_back <- 5
trainXY <- create_dataset(train, look_back)
testXY <-  create_dataset(test, look_back)
dim_train <- dim(trainXY$dataX)
dim_test <- dim(testXY$dataX)
# reshape input to be [samples, time steps, features]
dim(trainXY$dataX) <- c(dim_train[1], 1, dim_train[2])
dim(testXY$dataX) <- c(dim_test[1], 1, dim_test[2])
# Initialize the model
model <- keras_model_sequential()
# Train the model
trained_model <- model %>%
layer_lstm(units = 4,
input_shape = c(1, look_back)) %>%
layer_dense(units = 1) %>%
compile(loss = 'mean_squared_error',
optimizer = 'adam') %>%
fit(
trainXY$dataX,
trainXY$dataY,
epochs = 15,
batch_size = 73,
verbose = 1,
validation_split = 0.25
)
plot(trained_model)
# Initialize the model
model <- keras_model_sequential()
# Train the model
trained_model <- model %>%
layer_lstm(units = 4,
input_shape = c(1, look_back)) %>%
layer_dense(units = 1) %>%
compile(loss = 'mean_squared_error',
optimizer = 'adam') %>%
fit(
trainXY$dataX,
trainXY$dataY,
epochs = 15,
batch_size = 97,
verbose = 1,
validation_split = 0.25
)
plot(trained_model)
# Seperate training data and testing data
train <- dataset[1:train_size]
test <- dataset[(train_size + 1):length(dataset)]
look_back <- 5
trainXY <- create_dataset(train, look_back)
testXY <-  create_dataset(test, look_back)
dim_train <- dim(trainXY$dataX)
dim_test <- dim(testXY$dataX)
# reshape input to be [samples, time steps, features]
dim(trainXY$dataX) <- c(dim_train[1], 1, dim_train[2])
dim(testXY$dataX) <- c(dim_test[1], 1, dim_test[2])
# Initialize the model
model <- keras_model_sequential()
# Train the model
trained_model <- model %>%
layer_lstm(units = 4,
input_shape = c(1, look_back)) %>%
layer_dense(units = 1) %>%
compile(loss = 'mean_squared_error',
optimizer = 'adam') %>%
fit(
trainXY$dataX,
trainXY$dataY,
epochs = 20,
batch_size = 97,
verbose = 1,
validation_split = 0.25
)
plot(trained_model)
# Print out the test score
trainScore <- model %>%
evaluate(
trainXY$dataX,
trainXY$dataY,
verbose = 2)
testScore <- model %>%
evaluate(
testXY$dataX,
testXY$dataY,
verbose = 2)
sprintf(
'Train Score: %.4f MSE (%.4f RMSE)',
trainScore * spread^2,
sqrt(trainScore) * spread)
sprintf(
'Test Score: %.4f MSE (%.4f RMSE)',
testScore * spread^2,
sqrt(testScore) * spread)
# Fit the model to get predicted value
trainPredict <- model %>%
predict(trainXY$dataX,
verbose = 1)
testPredict <- model %>%
predict(testXY$dataX,
verbose = 1)
trainPredict <- trainPredict * spread + min_value
testPredict <- testPredict * spread + min_value
# Plot the output
df <- data.frame(
index = 1:length(dataset),
value = dataset * spread + min_value,
type = 'raw'
) %>%
rbind(data.frame(
index = 1:length(trainPredict) + look_back,
value = trainPredict,
type = 'train'
)) %>%
rbind(data.frame(
index = 1:length(testPredict) + look_back + length(train),
value = testPredict,
type = 'test'
))
ggplot(data = df) +
geom_line(mapping = aes(x = index,
y = value,
color = type)) +
geom_vline(xintercept = length(train) + 0.5) +
theme_minimal()
df %>%
mutate(index = ifelse(type %in% c("test", "train"), index - 2, index)) %>%
filter(index >= 1500,
index <= 1550) %>%
ggplot(aes(x = index,
y = value,
color = type)) +
geom_line()
df %>%
filter(index >= 1500,
index <= 1550) %>%
ggplot(aes(x = index,
y = value,
color = type)) +
geom_line()
