# 神经网络

## 环境

编程语言：python 3.9

库：pytorch

## NN

### 简介

&emsp;&emsp;神经网络，基于多层感知机，由输入层(input layer)，隐藏层(hidden layer)，输出层(output layer)。其中，每一层的连接伴随一个激活函数(active function)， 一般选择ReLU做为active function.具体结构如下图：

![preview](https://pic4.zhimg.com/v2-10867e40ac432b61d1aee8fff226617f_r.jpg)

&emsp;&emsp;其中，神经网络的训练通过反向传播算法进行，其本质为链式法则。

## 正则化

&emsp;&emsp;正则化一般用于防止过拟合问题的出现，以平衡误差与方差。经典的正则化方法有以下几种：

1. Dropout - 在全连接层中随机丢弃部分神经元节点，产生一个简化了的网络结构

2. L1/L2正则化 - 在原始的损失函数中增加L1/L2的惩罚项，从而限制产生较大的权重w
3. Batch normalization - 控制隐层的输出在一个稳定的范围内
4. 数据增强 - 通过增加数据集多样性的方式避免过拟合
5. Early stopping - 在达到模型过拟合的阶段前停止训练模型（没啥用）



## CNN

### ![卷积神经网络CNN完全指南终极版（二）](https://pic3.zhimg.com/v2-7ff93e3075ee6a73c9a0f56ba9a483db_1440w.jpg?source=172ae18b)

&emsp;&emsp;卷积神经网络，分为卷积层，池化层，常规的神经网络层。卷积为一类仿射运算，可以建立图片像素点之间的联系。具体调参的例子可以仿照花书P210图片。

## RNN

&emsp;&emsp;核心：共享参数，在一维序列上使用卷积，允许网络跨时间共享参数。

![img](https://pic1.zhimg.com/80/v2-206db7ba9d32a80ff56b6cc988a62440_1440w.jpg)



## LSTM

&emsp;&emsp;在RNN的基础上，增加了遗忘机制。

![A LSTM neural network.](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)

传统的LSTM增加了三个遗忘门（sigmoid函数）



## GRU

&emsp;&emsp;简化版的LSTM















